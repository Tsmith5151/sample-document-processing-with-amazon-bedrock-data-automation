{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cb5e4d-0d00-4ed6-9218-b0bbdc2cce83",
   "metadata": {},
   "source": [
    "# Operational Report Processing with Amazon Bedrock Data Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae9ae1-f5fe-4c88-9ea2-b692d447875e",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The energy industry generates large volumes of unstructured documents such as well permits, petrophysical logs, directional surveys, bit reports, frack data, and regulatory filings, just to name a few. For operators managing hundreds or thousands of wells, extracting actionable insights from these documents has traditionally been a manual, time-intensive process that limits operational efficiency and competitive intelligence gathering.\n",
    "\n",
    "Whether you're in energy, utilities, renewables, or any regulated industry and need to build a comprehensive data repository, or you're looking to extract competitive insights from public compliance documents, the challenge remains the same: how do you efficiently process and analyze thousands of PDF documents containing critical operational and technical information?\n",
    "\n",
    "This notebook demonstrates how to leverage Amazon Bedrock Data Automation to build custom extraction blueprints that automatically process complex, multimodal documents, transforming unstructured data into structured, analytics-ready datasets.\n",
    "\n",
    "### ‚ö†Ô∏è Challenge: Unstructured Data at Scale\n",
    "\n",
    "Challenges processing technical and operational documents: \n",
    "\n",
    "- Multiple document formats (PDFs, scanned images, legacy paper documents)\n",
    "- Inconsistent formatting and templates across business operations and service providers\n",
    "- Mix of handwritten notes, typed text, tables, and technical diagrams\n",
    "- Various quality levels of scanned documents\n",
    "- Industry-specific jargon and technical nomenclature\n",
    "- Complex numerical data in different units and formats\n",
    "\n",
    "### üöÄ Solution: Amazon Bedrock Data Automation\n",
    "\n",
    "Amazon Bedrock Data Automation provides a comprehensive solution for processing multimodal documents using foundation models. The service allows operators to:\n",
    "\n",
    "- **Create Custom Blueprints**: Define extraction schemas tailored to specific document types\n",
    "- **Process at Scale**: Handle thousands of documents through automated batch processing\n",
    "- **Maintain Accuracy**: Leverage advanced AI models for precise data extraction\n",
    "- **Integrate Seamlessly**: Output structured data directly to analytics platforms\n",
    "\n",
    "### ‚úÖ Key Benefits\n",
    "\n",
    "1. **Rapid Asset Integration**: Process acquisition documents in days instead of months\n",
    "2. **Competitive Intelligence**: Automatically extract insights from public regulatory data\n",
    "3. **Operational Efficiency**: Eliminate manual data entry and reduce processing errors\n",
    "4. **Scalable Processing**: Handle document volumes that would be impossible to process manually\n",
    "5. **Standardized Output**: Consistent data formats across different document sources\n",
    "\n",
    "\n",
    "### üéØ Goal\n",
    "\n",
    "This notebook demonstrates the power of automated document processing across multiple engineering reports from different operators in the energy sector. Using real-world examples of hydraulic fracturing reports submitted officially to the state, we'll focus on extracting critical operational data - including well information (report header), perforation and stimulation parameters - that are typically annotated on wellbore diagrams as shown below. The highlighted sections showcase key data points that traditionally require manual extraction, but can now be automatically identified and processed despite varying formats and nomenclature between operators. \n",
    "\n",
    "- Yellow box = perforation data\n",
    "- Blue box = stimulation data\n",
    "\n",
    "Through this example, we'll illustrate how Amazon Bedrock can intelligently handle these multi-page document variations, standardize the extracted information, and build a comprehensive operational database - all while maintaining accuracy across different document structures and naming convention.\n",
    "\n",
    "**Report Header**\n",
    "\n",
    "![Example1](assets/report_example.png)\n",
    "\n",
    "**Engineering Drawings (Wellbore Diagram)**\n",
    "\n",
    "![Example2](assets/report_compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6028a3a-db9d-437d-84af-315cc08722ce",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba05c4-1b9e-4581-97e1-a3f24fc900b9",
   "metadata": {},
   "source": [
    "The architecture diagram below illustrates an end-to-end solution leveraging Amazon Bedrock for automated document processing through a serverless workflow. When a user uploads a report to an S3 bucket, it triggers a Lambda function automatically. This Lambda function invokes a BDA processing jobs; the results are then stored back in a designated S3 bucket, creating a streamlined, event-driven pipeline for document automation. \n",
    "\n",
    "This design is particularly powerful because Bedrock's foundation models can handle various document formats and extract complex information without requiring extensive custom model training, making it ideal for processing technical and operational documents across different industries.\n",
    "\n",
    "For the purposes of the example presented in this notebook, we'll only focus on the Amazon Bedrock Data Automation steps to: create custom blueprint, create a BDA project, invoke a processing job, and then review the output results.\n",
    "\n",
    "![Architecture](assets/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3c462-eaaa-4b61-b5c1-a006736daa26",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "## üèÅ Getting Started\n",
    "\n",
    "Before proceeding with the steps below, check that you have\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Create a Bedrock Data Automation Profile IAM role. This role will have the necessary permissions for BDA to process documents and acts as the execution role that Bedrock assumes when running data automation jobs.\n",
    "- Necessary IAM permissions to create and manage Bedrock resources and read/write to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"boto3>=1.38.27\" \"pandas>=2.3.1\" \"s3fs==2025.5.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbf4c2-618c-4bd7-98a6-6014d3658cd9",
   "metadata": {},
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90a1f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_PROFILE = \"default\" \n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "if AWS_PROFILE != \"default\":\n",
    "    os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "    session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "else:\n",
    "    session = boto3.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "try:\n",
    "    sts = session.client('sts')\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"‚úÖ Connected to AWS\")\n",
    "    print(f\"‚úÖ Account ID: {identity['Account']}\")\n",
    "    print(f\"‚úÖ User/Role: {identity['Arn']}\")\n",
    "    print(f\"‚úÖ Region: {AWS_REGION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AWS connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "042b8298-80bd-4851-b60a-3799bef8efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# -- Import BDA and Helper functions -- \n",
    "from source import bda as bda_utils\n",
    "from source import utils as helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e527cb8-330f-449e-8106-3b986915c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BDA Parameters --\n",
    "env_name = 'dev'\n",
    "account_id = helper_utils.get_aws_account_id()\n",
    "s3_bucket_name = f\"energy-well-reports-{env_name}-{account_id}\"\n",
    "print(f'Using S3 bucket for BDA: {s3_bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3eb8b",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250cfef",
   "metadata": {},
   "source": [
    "This notebook leverages public data from the [Strategic Online Natural Resources Information System](sonris.com), maintained by the Louisiana Department of Energy and Natural Resources Engineering documents such as engineering reports and regulatory documents. To download the two reports previously submitted by each operator to SONRIS, run the following curl commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a142eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b38d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"https://sonlite.dnr.state.la.us/dnrservices/redirectUrl.jsp?dDocname=5171269&showInline=True\" -o \"data/reports/operator1_report.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88775541",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"https://sonlite.dnr.state.la.us/dnrservices/redirectUrl.jsp?dDocname=4107568&showInline=True\" -o \"data/reports/operator2_report.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbef8d-f8c4-41c5-b5ae-a966e9a73abb",
   "metadata": {},
   "source": [
    "## Step 1: Define Custom Blueprints\n",
    "\n",
    "Custom blueprints in Bedrock Data Automation allow users to create reusable, templated workflows that can be tailored to specific business requirements, ensuring consistent data processing patterns while reducing development time and maintaining governance standards across different projects and use cases. Through custom output configurations using blueprints, users can define precise extraction instructions that specify exactly which data points, fields, or content elements should be captured from documents, enabling tailored data extraction workflows that align with specific business requirements and downstream processing needs.\n",
    "\n",
    "When creating blueprints for BDA, here are some helpful best practices to consider:\n",
    "- Be explicit and detailed in blueprint names and descriptions to aid matching\n",
    "- Providing multiple relevant blueprints allows BDA to select the best match.\n",
    "- Create separate blueprints for significantly different document formats\n",
    "- Consider creating specialized blueprints for every vendor/document source if you need maximum accuracy\n",
    "- Do not include two blueprints of the same type in a project (e.g. two blueprints).\n",
    "- Information from the document itself and the blueprint is used to process documents, and including multiple blueprints of the same type in a project can lead to worse performance.\n",
    "\n",
    "\n",
    "In our scenario, reports can significantly vary by operations and vendor, requiring tailored BDA blueprints. Bedrock Data Automation supports up to 40 custom document blueprints per project, enabling unique extraction logic for diverse formats and styles. \n",
    "\n",
    "We'll focus on the two engineering reports previously downloaded, specific to completion operations. We'll create two distinct blueprints with instructions detailed instructions on the data extraction. Both custom blueprints are stored in the `data/blueprints` folder.\n",
    "\n",
    "The blueprint extracts comprehensive hydraulic fracturing completion data from multi-page well reports, including well identification details (API number, operator, field, location), perforation intervals, casing specifications, and detailed completion parameters for each fracturing stage (proppant volumes, pumping rates, pressures) on wellbore diagrams as shown below. It also captures water source information including supply types, volumes used, and source locations to provide a complete picture of the fracturing operation and environmental impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f207f-83d8-414c-aef2-5948238e34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Blueprint for Operator1 Report\n",
    "!cat data/blueprints/operator1_engineering_blueprint.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Blueprint for Operator2 Report \n",
    "!cat data/blueprints/operator2_engineering_blueprint.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27270381-12f7-4f93-b6df-d9e039abb3f4",
   "metadata": {},
   "source": [
    "## Step 2: Create Custom Blueprints\n",
    "\n",
    "Below is the core Python implementation that creates blueprints using the boto3 SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebbf2f-fa7f-4e5c-babb-ac4400dee6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "document_type = 'engineering'\n",
    "blueprint_arns = {}\n",
    "for operator in ['operator1','operator2']:\n",
    "    blueprint_arns[operator] = [bda_utils.create_custom_blueprint(f\"{operator}_{document_type}_blueprint\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef24515-7cca-42d1-be7a-09c07848d47a",
   "metadata": {},
   "source": [
    "## Step 3: Create Project - BDA\n",
    "\n",
    "The below `create_bda_project` creates a BDA project using the Python SDK, where you can associate your custom blueprint and configure the input data sources.\n",
    "\n",
    "**Note**: The [splitter feature](https://docs.aws.amazon.com/bedrock/latest/userguide/bda-document-splitting.html) defined in the `overrideConfiguration` intelligently breaks large documents into smaller, relevant chunks before processing with foundation models, which can significantly reduce token consumption and associated costs (See Bedrock Pricing [here](https://aws.amazon.com/bedrock/pricing/)). For instance, instead of processing an entire 20-page documents users can extract only the relevant sections/pages using a custom output and blueprint at `$0.040/page` and the remaining pages using the standard output cost tier of `$0.010/page`. Without enabling the splitter feature, the user would be charged the custom output price for all 20 pages. This feature becomes essential for enterprises handling large volumes of complex documents where only specific sections require analysis.\n",
    "\n",
    "BDA automatic splitting supports files with up to 3000 pages and supports individual documents of up to 20 pages each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d111568-beb2-4fe7-9dcf-299d6a15f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "project_arns = {}\n",
    "for operator, bp_arns in blueprint_arns.items():\n",
    "    project_arns[operator] = bda_utils.create_bda_project(f\"{operator}_{document_type}_project\", bp_arns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ddd8-9ad2-4194-9090-42bfb6421d75",
   "metadata": {},
   "source": [
    "## Step 4: Process Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d575d-19b1-49eb-874d-f6f5b10c5735",
   "metadata": {},
   "source": [
    "### Upload Report Files to \n",
    "\n",
    "Copy the sample well reports (from SONRIS) located in `data/reports` to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018507f-b465-43ae-9e7a-02a2f3e6d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "local_data_dir = 'data/reports'\n",
    "helper_utils.upload_data_to_s3(s3_bucket_name, local_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a24323-502d-4ca3-8807-322026e0322f",
   "metadata": {},
   "source": [
    "### Invoke Data Automation\n",
    "\n",
    "Next, invoke a BDA processing job to apply the custom blueprint to the well reports upload to S3, triggering the automated extraction workflow that will process your data according to the specifications defined in your project configuration. The processing job may take 1‚Äì2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9fe18-8541-455d-9a2d-d44606b43c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "bda_output_results_paths = []\n",
    "for operator, project_arn in project_arns.items():\n",
    "    file_name = os.path.join('reports',f'{operator}_report.pdf')\n",
    "    print(f'Using Project ARN: {project_arn} -- Processing file: {file_name}')\n",
    "    bda_output_results_paths.append(bda_utils.start_processing_job(project_arn, file_name, s3_bucket_name, wait_for_complete=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d3b27-bbea-4701-8fa4-8748db28a8cc",
   "metadata": {},
   "source": [
    "## Step 5: Review Results\n",
    "\n",
    "Lastly, explore the results by accessing the processed output data stored in the configured S3 output location, where you can review the extracted data points, validate the accuracy of the custom blueprint. The below steps explores the output from the BDA processing job for the engineering reports. We'll look at the results from each operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e261608f-d732-40b0-88ee-a18eca9168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Get S3 Paths --\n",
    "operator1_results_path,operator2_results_path = bda_output_results_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cd305-8afb-4e33-a8c2-eb3d1e164caf",
   "metadata": {},
   "source": [
    "### Operator1: Engineering Report\n",
    "\n",
    "Let's now explore the output results and cross-reference the extracted data with the engineering report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7ffc83f-feef-4665-8a72-63805fa26ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "operator1_custom_output_path =helper_utils.get_custom_output_path(operator1_results_path['S3_URI'])[0]\n",
    "data_op1 = helper_utils.get_s3_to_dict(operator1_custom_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8898349-e73b-469e-9492-099549d54a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op1['matched_blueprint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op1['inference_result'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43895872",
   "metadata": {},
   "source": [
    "![Example](assets/report1_header.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4518cf-d5ce-471f-a031-599427ec67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op1['inference_result']['Well_Information']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0460700",
   "metadata": {},
   "source": [
    "![Example](assets/report1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df09741-e275-49a5-9fe9-6a7a7a275d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_op1,'Completion_Summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a420507",
   "metadata": {},
   "source": [
    "### Operator2: Engineering Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1716346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "operator2_custom_output_path =helper_utils.get_custom_output_path(operator2_results_path['S3_URI'])[0]\n",
    "data_op2 = helper_utils.get_s3_to_dict(operator2_custom_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op2['matched_blueprint']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1b296",
   "metadata": {},
   "source": [
    "![Example](assets/report2_header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca43ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op2['inference_result']['Well_Information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_op2,'Casing_Summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfec80d",
   "metadata": {},
   "source": [
    "![Example](assets/report2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf627fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_op2,'Completion_Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_op2,'Perforation_Data').sort_values('Top Perf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo2",
   "language": "python",
   "name": "demo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
