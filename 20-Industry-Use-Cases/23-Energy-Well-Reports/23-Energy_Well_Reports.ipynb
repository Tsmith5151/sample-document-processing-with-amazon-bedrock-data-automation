{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cb5e4d-0d00-4ed6-9218-b0bbdc2cce83",
   "metadata": {},
   "source": [
    "# Operational Report Processing with Amazon Bedrock Data Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae9ae1-f5fe-4c88-9ea2-b692d447875e",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The energy industry generates large volumes of unstructured documents daily‚Äîfrom completion reports and directional surveys to bit reports, frack data, and regulatory filings. For operators managing hundreds or thousands of wells, extracting actionable insights from these documents has traditionally been a manual, time-intensive process that limits operational efficiency and competitive intelligence gathering.\n",
    "\n",
    "Whether you're an upstream or midstream operator who just completed an acquisition and needs to build a comprehensive well repository, or you're looking to extract competitive insights from public regulatory data, the challenge remains the same: how do you efficiently process and analyze thousands of PDF documents containing critical wellbore information?\n",
    "\n",
    "This notebook demonstrates how to leverage Amazon Bedrock Data Automation to build custom extraction blueprints that automatically process complex, multimodal documents, transforming unstructured data into structured, analytics-ready datasets.\n",
    "\n",
    "____\n",
    "\n",
    "### ‚ö†Ô∏è Challenge: Unstructured Data at Scale\n",
    "\n",
    "Oil and gas operators face several document processing challenges:\n",
    "\n",
    "**Internal Operations:**\n",
    "- **Post-Acquisition Integration**: Newly acquired assets often come with thousands of legacy documents in various formats\n",
    "- **Well Profile Development**: Critical wellbore data is embedded in schematic drawings and technical reports\n",
    "- **Operational Analytics**: Performance data scattered across completion reports, workover records, and artificial lift documentation\n",
    "\n",
    "**External Intelligence:**\n",
    "- **Competitive Analysis**: Public regulatory data provides market insights but exists in unstructured PDF formats\n",
    "- **Regulatory Compliance**: Permit data, EIA filings, and state regulatory documents require manual review\n",
    "- **Market Research**: SONRIS, FracFocus, and other public databases contain valuable intelligence locked in document formats\n",
    "\n",
    "### üöÄ Solution: Amazon Bedrock Data Automation\n",
    "\n",
    "Amazon Bedrock Data Automation provides a comprehensive solution for processing multimodal documents using foundation models. The service allows operators to:\n",
    "\n",
    "- **Create Custom Blueprints**: Define extraction schemas tailored to specific document types\n",
    "- **Process at Scale**: Handle thousands of documents through automated batch processing\n",
    "- **Maintain Accuracy**: Leverage advanced AI models for precise data extraction\n",
    "- **Integrate Seamlessly**: Output structured data directly to analytics platforms\n",
    "\n",
    "### ‚úÖ Key Benefits for Oil & Gas Operations\n",
    "\n",
    "1. **Rapid Asset Integration**: Process acquisition documents in days instead of months\n",
    "2. **Competitive Intelligence**: Automatically extract insights from public regulatory data\n",
    "3. **Operational Efficiency**: Eliminate manual data entry and reduce processing errors\n",
    "4. **Scalable Processing**: Handle document volumes that would be impossible to process manually\n",
    "5. **Standardized Output**: Consistent data formats across different document sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6028a3a-db9d-437d-84af-315cc08722ce",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba05c4-1b9e-4581-97e1-a3f24fc900b9",
   "metadata": {},
   "source": [
    "![Architecture](assets/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e4d15-f726-4d3d-bb04-b2c7664ebf8b",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In the following steps, we'll walkthrough using Amazon Bedrock Data Automation to automate extracting data from operational well reports. In this example, we'll leverage public data from the [Strategic Online Natural Resources Information System](sonris.com), maintained by the Louisiana Department of Energy and Natural Resources Engineering documents such as completion reports contain detailed data on perforation intervals, tubing set points, hydraulic fracturing designs, and initial production outcomes ‚Äî key insights for optimizing or designing high-performing wells.\n",
    "\n",
    "### Example: Well Completions Report  \n",
    "\n",
    "![Example](assets/report_example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3c462-eaaa-4b61-b5c1-a006736daa26",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "## üèÅ Prerequisites\n",
    "\n",
    "Before proceeding with the steps below, check that you have\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Create a Bedrock Data Automation Profile IAM role. This role will have the necessary permissions for BDA to process documents and acts as the execution role that Bedrock assumes when running data automation jobs.\n",
    "- Necessary IAM permissions to create and manage Bedrock resources and read/write to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea63326-3efc-48ed-aa20-5130f012788a",
   "metadata": {},
   "source": [
    "### Configure Notebook Environment\n",
    "\n",
    "This project leverages [uv](https://docs.astral.sh/uv/) to manage Python dependencies. To get started, please refer to the documentation on installing `uv` [here](https://docs.astral.sh/uv/getting-started/installation/). \n",
    "\n",
    "Next, open a terminal and run the following commands to create a dedicated kernel for this notebook environment. \n",
    "\n",
    "1. **Install Python**\n",
    "    ```\n",
    "    uv python install 3.12\n",
    "    ```\n",
    "\n",
    "2. **Create Virtual Environment**\n",
    "    ```\n",
    "    uv venv --python 3.12\n",
    "    ```\n",
    "\n",
    "3. **Activate Virtual Environment**\n",
    "    ```bash\n",
    "    source .venv/bin/activate\n",
    "    ```\n",
    "\n",
    "4. **Install Dependencies**\n",
    "    ```bash\n",
    "    uv syc\n",
    "    ```\n",
    "\n",
    "5. **Install ipykernel**\n",
    "\n",
    "    ```bash\n",
    "    uv add --dev ipykernel\n",
    "    ```\n",
    "\n",
    "6. **Create Kernel for Notebook**\n",
    "    ```bash\n",
    "    uv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=bda_kernel\n",
    "    ```\n",
    "\n",
    "Next, restart the kernel and then change to the newly created kernel: Select `Kernel` -> `Change Kernel` -> Select `bda_kernel` kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbf4c2-618c-4bd7-98a6-6014d3658cd9",
   "metadata": {},
   "source": [
    "### Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90a1f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_PROFILE = \"default\" \n",
    "AWS_REGION = 'us-east-1'\n",
    "\n",
    "if AWS_PROFILE != \"default\":\n",
    "    os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "    session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "else:\n",
    "    session = boto3.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "try:\n",
    "    sts = session.client('sts')\n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"‚úÖ Connected to AWS\")\n",
    "    print(f\"‚úÖ Account ID: {identity['Account']}\")\n",
    "    print(f\"‚úÖ User/Role: {identity['Arn']}\")\n",
    "    print(f\"‚úÖ Region: {AWS_REGION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AWS connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "042b8298-80bd-4851-b60a-3799bef8efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# -- Import BDA and Helper functions -- \n",
    "from source import bda as bda_utils\n",
    "from source import utils as helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e527cb8-330f-449e-8106-3b986915c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BDA Parameters --\n",
    "env_name = 'dev'\n",
    "account_id = helper_utils.get_aws_account_id()\n",
    "s3_bucket_name = f\"energy-well-reports-{env_name}-{account_id}\"\n",
    "project_name = \"energy-well-reports-bda\"\n",
    "\n",
    "print(f'BDA Project {project_name} configured using S3 bucket {s3_bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbef8d-f8c4-41c5-b5ae-a966e9a73abb",
   "metadata": {},
   "source": [
    "## Step 1: Define Custom Blueprints\n",
    "\n",
    "Custom blueprints in Bedrock Data Automation allow users to create reusable, templated workflows that can be tailored to specific business requirements, ensuring consistent data processing patterns while reducing development time and maintaining governance standards across different projects and use cases. Through custom output configurations using blueprints, users can define precise extraction instructions that specify exactly which data points, fields, or content elements should be captured from documents, enabling tailored data extraction workflows that align with specific business requirements and downstream processing needs.\n",
    "\n",
    "When creating blueprints for BDA, here are some helpful best practices to consider:\n",
    "- Be explicit and detailed in blueprint names and descriptions to aid matching\n",
    "- Providing multiple relevant blueprints allows BDA to select the best match.\n",
    "- Create separate blueprints for significantly different document formats\n",
    "- Consider creating specialized blueprints for every vendor/document source if you need maximum accuracy\n",
    "- Do not include two blueprints of the same type in a project (e.g. two completion blueprints).\n",
    "- Information from the document itself and the blueprint is used to process documents, and including multiple blueprints of the same type in a project can lead to worse performance.\n",
    "\n",
    "\n",
    "In our scenario, reports can significantly vary by operations and vendor, requiring tailored BDA blueprints. Bedrock Data Automation supports up to 40 custom document blueprints per project, enabling unique extraction logic for diverse formats and styles. \n",
    "\n",
    "We'll focus on two commono operation reports in the energy sector, specific to oil and gas upstream operations. We'll create two distinct blueprints with instructions detailed instructions on the data extraction. Both custom blueprints are stored in the `data/blueprints` folder.\n",
    "\n",
    "### I. Completion Report Blueprint\n",
    "\n",
    "The `completions` blueprint extracts comprehensive hydraulic fracturing completion data from multi-page well reports, including well identification details (API number, operator, field, location), perforation intervals, casing specifications, and detailed completion parameters for each fracturing stage (proppant volumes, pumping rates, pressures) on wellbore diagrams as shown below. It also captures water source information including supply types, volumes used, and source locations to provide a complete picture of the fracturing operation and environmental impact.\n",
    "\n",
    "![Example](assets/completions_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f207f-83d8-414c-aef2-5948238e34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Blueprint:\n",
    "!cat data/blueprints/completions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4853cb8-e074-40fc-a754-7f1b2d2a9cf6",
   "metadata": {},
   "source": [
    "### II. Directional Survey Blueprint\n",
    "\n",
    "The `directional` blueprint extracts comprehensive directional survey data from well reports, including header information (operator, field, well name, survey provider), coordinate reference systems, and detailed survey measurement points with depth, inclination, azimuth, and coordinate data. It also captures summary statistics like total measured depth, maximum inclination, and API number to provide a complete picture of the wellbore trajectory and drilling parameters.\n",
    "\n",
    "![Example](assets/directional_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21c461-01cc-4741-bf31-1afbe57fb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Blueprint:\n",
    "!cat data/blueprints/directional.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27270381-12f7-4f93-b6df-d9e039abb3f4",
   "metadata": {},
   "source": [
    "## Step 2: Create Custom Blueprints\n",
    "\n",
    "Below is the core Python implementation that creates blueprints using the boto3 SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebbf2f-fa7f-4e5c-babb-ac4400dee6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "blueprint_arns = bda_utils.create_custom_blueprint(blueprint_names=[\"completions\", \"directional\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef24515-7cca-42d1-be7a-09c07848d47a",
   "metadata": {},
   "source": [
    "## Step 3: Create Project - BDA\n",
    "\n",
    "The below `create_bda_project` creates a BDA project using the Python SDK, where you can associate your custom blueprint and configure the input data sources.\n",
    "\n",
    "**Note**: The [splitter feature](https://docs.aws.amazon.com/bedrock/latest/userguide/bda-document-splitting.html) defined in the `overrideConfiguration` intelligently breaks large documents into smaller, relevant chunks before processing with foundation models, which can significantly reduce token consumption and associated costs (See Bedrock Pricing [here](https://aws.amazon.com/bedrock/pricing/)). For instance, instead of processing an entire 20-page documents users can extract only the relevant sections/pages using a custom output and blueprint at `$0.040/page` and the remaining pages using the standard output cost tier of `$0.010/page`. Without enabling the splitter feature, the user would be charged the custom output price for all 20 pages. This feature becomes essential for enterprises handling large volumes of complex documents where only specific sections require analysis.\n",
    "\n",
    "BDA automatic splitting supports files with up to 3000 pages and supports individual documents of up to 20 pages each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d111568-beb2-4fe7-9dcf-299d6a15f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "project_arn = bda_utils.create_bda_project(project_name, blueprint_arns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ddd8-9ad2-4194-9090-42bfb6421d75",
   "metadata": {},
   "source": [
    "## Step 4: Process Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d575d-19b1-49eb-874d-f6f5b10c5735",
   "metadata": {},
   "source": [
    "### Upload Report Files to \n",
    "\n",
    "Copy the sample well reports (from SONRIS) located in `data/reports` to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018507f-b465-43ae-9e7a-02a2f3e6d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "local_data_dir = 'data/reports'\n",
    "helper_utils.upload_data_to_s3(s3_bucket_name, local_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a24323-502d-4ca3-8807-322026e0322f",
   "metadata": {},
   "source": [
    "### Invoke Data Automation\n",
    "\n",
    "Next, invoke a BDA processing job to apply the custom blueprint to the well reports upload to S3, triggering the automated extraction workflow that will process your data according to the specifications defined in your project configuration. The processing job may take 1‚Äì2 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9fe18-8541-455d-9a2d-d44606b43c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "bda_output_results_paths = []\n",
    "files_to_process = helper_utils.list_s3_files(s3_bucket_name,'reports')\n",
    "for file_name in files_to_process:\n",
    "    print(f'Processing file: {file_name}')\n",
    "    bda_output_results_paths.append(bda_utils.start_processing_job(project_arn, file_name, s3_bucket_name, wait_for_complete=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d3b27-bbea-4701-8fa4-8748db28a8cc",
   "metadata": {},
   "source": [
    "## Step 5: Review Results\n",
    "\n",
    "Lastly, explore the results by accessing the processed output data stored in the configured S3 output location, where you can review the extracted data points, validate the accuracy of the custom blueprint. The below is the output of the BDA processing job for the completion and directional report types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e261608f-d732-40b0-88ee-a18eca9168bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --  Get S3 Paths --\n",
    "completions_output_path,directional_output_path = bda_output_results_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cd305-8afb-4e33-a8c2-eb3d1e164caf",
   "metadata": {},
   "source": [
    "### Completions Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7ffc83f-feef-4665-8a72-63805fa26ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_custom_output_path =helper_utils.get_custom_output_path(completions_output_path['S3_URI'])\n",
    "data_c = helper_utils.get_s3_to_dict(completions_custom_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8898349-e73b-469e-9492-099549d54a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c['matched_blueprint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1a0e1-658e-41ff-9303-c30866d75347",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c['inference_result'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4518cf-d5ce-471f-a031-599427ec67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c['inference_result']['Well_Information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df09741-e275-49a5-9fe9-6a7a7a275d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_c,'Completion_Summary').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb7648-73c4-42d8-82f4-8159da360751",
   "metadata": {},
   "source": [
    "### Directional Survey Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8036b5d8-e551-4573-bcf7-5f70095da050",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_custom_output_path = helper_utils.get_custom_output_path(directional_output_path['S3_URI'])\n",
    "data_d = helper_utils.get_s3_to_dict(directional_custom_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71000f-5f5a-4f1d-8f45-a5c20ab24de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d['matched_blueprint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282e59d-41c5-4f0e-b864-4f6dc6ab24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d['inference_result'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d135970-730e-43b8-ba75-5e42f17e04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d['inference_result']['Header_Information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dded8-68d8-4d63-8417-b0782afe5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.get_dataframe(data_d,'Directional_Survey_Data').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bbd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo2",
   "language": "python",
   "name": "demo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
